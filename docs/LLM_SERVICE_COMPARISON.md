# LLM ì„œë¹„ìŠ¤ ë¹„êµ: vLLM vs Ollama

**ì‘ì„±ì¼**: 2026-01-26  
**ë²„ì „**: 1.3

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [ì¼€ì´ìŠ¤ A: vLLM](#2-ì¼€ì´ìŠ¤-a-vllm)
3. [ì¼€ì´ìŠ¤ B: Ollama](#3-ì¼€ì´ìŠ¤-b-ollama)
4. [API ì°¨ì´ì  ë¹„êµ](#4-api-ì°¨ì´ì -ë¹„êµ)
5. [Backend êµ¬í˜„ ì°¨ì´ì ](#5-backend-êµ¬í˜„-ì°¨ì´ì )
6. [í™˜ê²½ ë³€ìˆ˜ ì„¤ì •](#6-í™˜ê²½-ë³€ìˆ˜-ì„¤ì •)

---

## 1. ê°œìš”

**âš ï¸ ì¤‘ìš”**: vLLMê³¼ OllamaëŠ” ë™ì‹œì— ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (VRAM ì œì•½). í•˜ë‚˜ë§Œ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.

### ì„ íƒ ê¸°ì¤€

| í•­ëª© | vLLM | Ollama |
|------|------|--------|
| **ëª¨ë¸ í˜•ì‹** | BitsAndBytes 4-bit | GGUF (Q4_K_XL) |
| **ìš©ëŸ‰** | ~14GB | ~16.8GB |
| **ì„±ëŠ¥** | ë†’ì€ ì²˜ë¦¬ëŸ‰ (ë™ì‹œ ì ‘ì† 10-12ëª…) | ì¤‘ê°„ ì²˜ë¦¬ëŸ‰ |
| **API í‘œì¤€** | OpenAI í˜¸í™˜ | Ollama ìì²´ API |
| **ì„¤ì • ë³µì¡ë„** | ì¤‘ê°„ | ë‚®ìŒ |
| **ì¶”ì²œ ìš©ë„** | í”„ë¡œë•ì…˜, ë†’ì€ ë™ì‹œ ì ‘ì† | ê°œë°œ/í…ŒìŠ¤íŠ¸, ê°„ë‹¨í•œ ì„¤ì • |

---

## 2. ì¼€ì´ìŠ¤ A: vLLM

### 2.1 ì„œë²„ ì„¤ì •

**í¬íŠ¸**: 8002 (ì™¸ë¶€) â†’ 8000 (ë‚´ë¶€ ì»¨í…Œì´ë„ˆ)  
**ê¸°ë³¸ URL**: `http://server-a:8002` ë˜ëŠ” `http://localhost:8002`  
**API í‘œì¤€**: OpenAI í˜¸í™˜ API

### 2.2 Docker Compose ì„¤ì •

```yaml
# docker-compose.yml (vLLM ì‚¬ìš© ì‹œ)
services:
  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    ports:
      - "8002:8000"
    command:
      - unsloth/gemma-3-27b-it-bnb-4bit
      - --tensor-parallel-size 1
      - --dtype auto
      - --quantization bitsandbytes
      - --max-model-len 4096
```

### 2.3 API ì—”ë“œí¬ì¸íŠ¸

#### `POST /v1/chat/completions`

**ìš”ì²­ í˜•ì‹**:
```json
{
  "model": "unsloth/gemma-3-27b-it-bnb-4bit",
  "messages": [
    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}
  ],
  "max_tokens": 512,
  "temperature": 0.7
}
```

**ì‘ë‹µ í˜•ì‹**:
```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "unsloth/gemma-3-27b-it-bnb-4bit",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 20,
    "total_tokens": 30
  }
}
```

#### `GET /v1/models`

**ì‘ë‹µ í˜•ì‹**:
```json
{
  "object": "list",
  "data": [
    {
      "id": "unsloth/gemma-3-27b-it-bnb-4bit",
      "object": "model",
      "created": 1769364590,
      "owned_by": "vllm"
    }
  ]
}
```

---

## 3. ì¼€ì´ìŠ¤ B: Ollama

### 3.1 ì„œë²„ ì„¤ì •

**í¬íŠ¸**: 11434  
**ê¸°ë³¸ URL**: `http://server-a:11434` ë˜ëŠ” `http://localhost:11434`  
**API í‘œì¤€**: Ollama ìì²´ API

### 3.2 Docker Compose ì„¤ì •

```yaml
# docker-compose.yml (Ollama ì‚¬ìš© ì‹œ)
services:
  ollama-server:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - /mnt/shared_models/llm/gemma-3-27b-it-GGUF:/models:ro
      - ollama-data:/root/.ollama
```

### 3.3 ëª¨ë¸ ë“±ë¡

```bash
# Ollama ì‹¤í–‰ í›„ ëª¨ë¸ ë“±ë¡
docker exec -it ollama-server ollama create gemma-3-27b-it -f /models/gemma-3-27b-it-UD-Q4_K_XL.gguf
```

### 3.4 API ì—”ë“œí¬ì¸íŠ¸

**âš ï¸ ì¤‘ìš”**: ì±„íŒ… ê¸°ëŠ¥ì—ëŠ” `/api/chat`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. `/api/generate`ëŠ” ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ë§Œ ì§€ì›í•˜ë¯€ë¡œ ì±„íŒ…ì— ë¶€ì í•©í•©ë‹ˆë‹¤.

#### `POST /api/chat` âœ… **ì±„íŒ…ìš© (ê¶Œì¥)**

**ìš”ì²­ í˜•ì‹**:
```json
{
  "model": "gemma-3-27b-it",
  "messages": [
    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}
  ],
  "stream": false,
  "options": {
    "temperature": 0.7,
    "num_predict": 512
  }
}
```

**ì‘ë‹µ í˜•ì‹**:
```json
{
  "model": "gemma-3-27b-it",
  "created_at": "2026-01-26T12:00:00Z",
  "message": {
    "role": "assistant",
    "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
  },
  "done": true,
  "total_duration": 1234567890,
  "load_duration": 1234567,
  "prompt_eval_count": 10,
  "prompt_eval_duration": 1234567,
  "eval_count": 20,
  "eval_duration": 1234567890
}
```

#### `GET /api/tags`

**ì‘ë‹µ í˜•ì‹** (ê³µì‹ ë¬¸ì„œ ê¸°ì¤€):
```json
{
  "models": [
    {
      "name": "gemma-3-27b-it",
      "modified_at": "2026-01-26T12:00:00Z",
      "size": 16800000000,
      "digest": "sha256:abc123...",
      "details": {
        "format": "gguf",
        "family": "gemma",
        "families": ["gemma"],
        "parameter_size": "27B",
        "quantization_level": "Q4_K_XL"
      }
    }
  ]
}
```

#### `POST /api/generate` âš ï¸ **ì°¸ê³ ìš© (ì±„íŒ…ì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)**

**ìš©ë„**: ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± (ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ë¯¸ì§€ì›)

**ìš”ì²­ í˜•ì‹**:
```json
{
  "model": "gemma-3-27b-it",
  "prompt": "ì•ˆë…•í•˜ì„¸ìš”",
  "stream": false,
  "options": {
    "temperature": 0.7,
    "num_predict": 512
  }
}
```

**ì‘ë‹µ í˜•ì‹**:
```json
{
  "model": "gemma-3-27b-it",
  "created_at": "2026-01-26T12:00:00Z",
  "response": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
  "done": true,
  "prompt_eval_count": 10,
  "eval_count": 20
}
```

**âš ï¸ ì£¼ì˜**: `/api/generate`ëŠ” `prompt`ë§Œ ì‚¬ìš©í•˜ê³  `messages`ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì±„íŒ… ê¸°ëŠ¥ì—ëŠ” `/api/chat`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

---

## 4. API ì°¨ì´ì  ë¹„êµ

### 4.1 ì—”ë“œí¬ì¸íŠ¸ ë¹„êµ

| ê¸°ëŠ¥ | vLLM | Ollama |
|------|------|--------|
| **ì±„íŒ… ì™„ë£Œ** | `POST /v1/chat/completions` | `POST /api/chat` (ê¶Œì¥) |
| **í…ìŠ¤íŠ¸ ìƒì„±** | `POST /v1/completions` | `POST /api/generate` (ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ë§Œ) |
| **ëª¨ë¸ ëª©ë¡** | `GET /v1/models` | `GET /api/tags` |
| **í—¬ìŠ¤ì²´í¬** | `GET /health` | `GET /api/version` |

### 4.2 ìš”ì²­ í˜•ì‹ ë¹„êµ

#### vLLM ìš”ì²­
```json
{
  "model": "unsloth/gemma-3-27b-it-bnb-4bit",
  "messages": [...],
  "temperature": 0.7,
  "max_tokens": 512
}
```

#### Ollama ìš”ì²­
```json
{
  "model": "gemma-3-27b-it",
  "messages": [...],
  "stream": false,
  "options": {
    "temperature": 0.7,
    "num_predict": 512
  }
}
```

**ì£¼ìš” ì°¨ì´ì **:
- vLLM: `max_tokens` ì§ì ‘ ì‚¬ìš©
- Ollama: `options.num_predict` ì‚¬ìš©
- Ollama: `stream` í•„ë“œ í•„ìˆ˜

### 4.3 ì‘ë‹µ í˜•ì‹ ë¹„êµ

#### vLLM ì‘ë‹µ
```json
{
  "choices": [{
    "message": {"role": "assistant", "content": "..."}
  }],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 20
  }
}
```

#### Ollama ì‘ë‹µ
```json
{
  "message": {"role": "assistant", "content": "..."},
  "prompt_eval_count": 10,
  "eval_count": 20
}
```

**ì£¼ìš” ì°¨ì´ì **:
- vLLM: `choices[0].message.content`
- Ollama: `message.content`
- vLLM: `usage.prompt_tokens`, `usage.completion_tokens`
- Ollama: `prompt_eval_count`, `eval_count`

---

## 5. Backend êµ¬í˜„ ì°¨ì´ì 

### 5.1 Server B Backend ì½”ë“œ êµ¬í˜„ ìƒíƒœ

**ìœ„ì¹˜**: `server-b/backend/app/api/chat.py`

**í˜„ì¬ ìƒíƒœ**: âœ… Ollama ê¸°ì¤€ìœ¼ë¡œ êµ¬í˜„ ì™„ë£Œ (vLLM ì½”ë“œëŠ” ì£¼ì„ ì²˜ë¦¬ë¨)

#### ì¼€ì´ìŠ¤ B: Ollama ì‚¬ìš© (ê¸°ë³¸, í˜„ì¬ êµ¬í˜„ë¨)

```python
# í™˜ê²½ ë³€ìˆ˜
LLM_SERVICE = os.getenv("LLM_SERVICE", "ollama")  # ê¸°ë³¸ê°’: ollama
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://server-a:11434")

# API í˜¸ì¶œ
async with httpx.AsyncClient() as client:
    response = await client.post(
        f"{OLLAMA_BASE_URL}/api/chat",
        json={
            "model": request.model,
            "messages": messages,
            "stream": False,  # OllamaëŠ” stream í•„ë“œ í•„ìˆ˜
            "options": {
                "temperature": request.temperature,
                "num_predict": request.max_tokens  # OllamaëŠ” num_predict ì‚¬ìš©
            }
        },
        timeout=60.0
    )
    result = response.json()
    
    # ì‘ë‹µ íŒŒì‹± (í•„ë“œëª… ë³€í™˜)
    content = result["message"]["content"]
    usage = {
        "prompt_tokens": result.get("prompt_eval_count", 0),
        "completion_tokens": result.get("eval_count", 0)
    }
```

#### ì¼€ì´ìŠ¤ A: vLLM ì‚¬ìš© (ì£¼ì„ ì²˜ë¦¬ë¨, í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)

```python
# vLLM ì½”ë“œëŠ” chat.pyì— ì£¼ì„ìœ¼ë¡œ ë³´ê´€ë˜ì–´ ìˆìŒ
# í•„ìš” ì‹œ ì£¼ì„ì„ í•´ì œí•˜ê³  LLM_SERVICEë¥¼ "vllm"ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì‚¬ìš©

# í™˜ê²½ ë³€ìˆ˜
# LLM_SERVICE = os.getenv("LLM_SERVICE", "vllm")
# VLLM_BASE_URL = os.getenv("VLLM_BASE_URL", "http://server-a:8002")

# API í˜¸ì¶œ
# async with httpx.AsyncClient() as client:
#     response = await client.post(
#         f"{VLLM_BASE_URL}/v1/chat/completions",
#         json={
#             "model": request.model,
#             "messages": messages,
#             "temperature": request.temperature,
#             "max_tokens": request.max_tokens
#         },
#         timeout=60.0
#     )
#     result = response.json()
#     
#     # ì‘ë‹µ íŒŒì‹±
#     content = result["choices"][0]["message"]["content"]
#     usage = {
#         "prompt_tokens": result["usage"]["prompt_tokens"],
#         "completion_tokens": result["usage"]["completion_tokens"]
#     }
```

### 5.2 í˜ë¥´ì†Œë‚˜ í¬ë§·íŒ… ë° TTS í†µí•© âœ…

**ìœ„ì¹˜**: `server-b/backend/app/api/chat.py`

**êµ¬í˜„ ì™„ë£Œëœ ê¸°ëŠ¥**:
- âœ… `format_persona_for_roleplay()` í•¨ìˆ˜: ì—­í• ê·¹ì— ì í•©í•œ êµ¬ì¡°í™”ëœ í˜ë¥´ì†Œë‚˜ í¬ë§·íŒ…
- âœ… ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ í¬í•¨: opponent, situation, background
- âœ… Character ì¡°íšŒ: character_idë¡œ voice_id ìë™ ì¶”ì¶œ
- âœ… TTS í†µí•©: ì±„íŒ… ì‘ë‹µ í›„ ìë™ TTS í˜¸ì¶œ (`_synthesize_tts_internal` ì‚¬ìš©)
- âœ… ë§¤ ìš”ì²­ë§ˆë‹¤ system ë©”ì‹œì§€ í¬í•¨ (Ollama ìš”êµ¬ì‚¬í•­)

**í˜ë¥´ì†Œë‚˜ í¬ë§·íŒ… ì˜ˆì‹œ**:
```python
formatted_persona = format_persona_for_roleplay(
    persona=request.persona,
    character_name=character.name if character else None,
    scenario=request.scenario
)
# ê²°ê³¼: "ë‹¹ì‹ ì€ {character_name}ì…ë‹ˆë‹¤.\n{persona}\n\ní˜„ì¬ ìƒí™©: {situation}..."
```

### 5.3 í˜„ì¬ êµ¬í˜„ ìƒíƒœ

**ìœ„ì¹˜**: `server-b/backend/app/api/chat.py`

**í˜„ì¬ êµ¬í˜„**: âœ… Ollama ê¸°ì¤€ìœ¼ë¡œ êµ¬í˜„ ì™„ë£Œ (vLLM ì½”ë“œëŠ” ì£¼ì„ ì²˜ë¦¬ë¨)

```python
# server-b/backend/app/api/chat.py
# í˜„ì¬ ê¸°ë³¸ê°’: ollama
LLM_SERVICE = os.getenv("LLM_SERVICE", "ollama")  # ê¸°ë³¸ê°’: ollama

async def call_llm_service(
    messages: List[Dict[str, str]],
    model: str,
    temperature: float = 0.7,
    max_tokens: int = 512
) -> Dict[str, Any]:
    """LLM ì„œë¹„ìŠ¤ í˜¸ì¶œ (Ollama ê¸°ì¤€, vLLM ì½”ë“œëŠ” ì£¼ì„ ì²˜ë¦¬ë¨)"""
    
    if LLM_SERVICE == "ollama":
        # ì¼€ì´ìŠ¤ B: Ollama API (ê¸°ë³¸ ì‚¬ìš©)
        OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://server-a:11434")
        response = await client.post(
            f"{OLLAMA_BASE_URL}/api/chat",
            json={
                "model": model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            },
            timeout=60.0
        )
        result = response.json()
        return {
            "content": result["message"]["content"],
            "usage": {
                "prompt_tokens": result.get("prompt_eval_count", 0),
                "completion_tokens": result.get("eval_count", 0)
            }
        }
    
    # vLLM ì½”ë“œëŠ” ì£¼ì„ ì²˜ë¦¬ë˜ì–´ ìˆìŒ (í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
    # elif LLM_SERVICE == "vllm":
    #     VLLM_BASE_URL = os.getenv("VLLM_BASE_URL", "http://server-a:8002")
    #     response = await client.post(
    #         f"{VLLM_BASE_URL}/v1/chat/completions",
    #         json={
    #             "model": model,
    #             "messages": messages,
    #             "temperature": temperature,
    #             "max_tokens": max_tokens
    #         },
    #         timeout=60.0
    #     )
    #     result = response.json()
    #     return {
    #         "content": result["choices"][0]["message"]["content"],
    #         "usage": {
    #             "prompt_tokens": result["usage"]["prompt_tokens"],
    #             "completion_tokens": result["usage"]["completion_tokens"]
    #         }
    #     }
```

---

## 6. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

### 6.1 Server B Backend í™˜ê²½ ë³€ìˆ˜

**íŒŒì¼**: `server-b/backend/.env`

#### ì¼€ì´ìŠ¤ B: Ollama ì‚¬ìš© (ê¸°ë³¸, í˜„ì¬ êµ¬í˜„ë¨)

```bash
# LLM ì„œë¹„ìŠ¤ ì„ íƒ (ê¸°ë³¸ê°’: ollama)
LLM_SERVICE=ollama

# Ollama ì„œë²„ URL
OLLAMA_BASE_URL=http://gpugpt.duckdns.org  # ë˜ëŠ” ì§ì ‘ IP (ì˜ˆ: http://192.168.1.100:11434)
```

#### ì¼€ì´ìŠ¤ A: vLLM ì‚¬ìš© (ì£¼ì„ ì²˜ë¦¬ë¨, í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)

```bash
# vLLM ì‚¬ìš© ì‹œ chat.pyì˜ vLLM ì½”ë“œ ì£¼ì„ í•´ì œ í•„ìš”
# LLM_SERVICE=vllm

# vLLM ì„œë²„ URL (ì£¼ì„ ì²˜ë¦¬ëœ ì½”ë“œì—ì„œ ì‚¬ìš©)
# VLLM_BASE_URL=http://server-a:8002
```

### 6.2 Server A Docker Compose

#### ì¼€ì´ìŠ¤ A: vLLM ì‹¤í–‰

```bash
docker-compose --profile vllm up -d vllm-server
```

#### ì¼€ì´ìŠ¤ B: Ollama ì‹¤í–‰

```bash
docker-compose --profile ollama up -d ollama-server
```

---

## 7. ìš”ì•½

### ì£¼ìš” ì°¨ì´ì 

1. **API ì—”ë“œí¬ì¸íŠ¸**:
   - vLLM: `/v1/chat/completions` (OpenAI í˜¸í™˜)
   - Ollama: `/api/chat` (Ollama ìì²´)

2. **ìš”ì²­ í˜•ì‹**:
   - vLLM: `max_tokens` ì§ì ‘ ì‚¬ìš©
   - Ollama: `options.num_predict` ì‚¬ìš©

3. **ì‘ë‹µ í˜•ì‹**:
   - vLLM: `choices[0].message.content`
   - Ollama: `message.content`

4. **í† í° ì‚¬ìš©ëŸ‰**:
   - vLLM: `usage.prompt_tokens`, `usage.completion_tokens`
   - Ollama: `prompt_eval_count`, `eval_count`

5. **í¬íŠ¸**:
   - vLLM: 8002
   - Ollama: 11434

### êµ¬í˜„ ì‹œ ì£¼ì˜ì‚¬í•­

1. **í™˜ê²½ ë³€ìˆ˜ë¡œ ì„ íƒ**: `LLM_SERVICE=vllm` ë˜ëŠ” `LLM_SERVICE=ollama`
2. **ì‘ë‹µ íŒŒì‹± ë¶„ê¸°**: ì¼€ì´ìŠ¤ë³„ë¡œ ë‹¤ë¥¸ ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬
3. **í† í° ì‚¬ìš©ëŸ‰ ë³€í™˜**: Ollamaì˜ ê²½ìš° í•„ë“œëª… ë³€í™˜ í•„ìš”
4. **ëª¨ë¸ ì´ë¦„**: vLLMì€ Hugging Face ëª¨ë¸ ID, OllamaëŠ” ë“±ë¡ëœ ëª¨ë¸ ì´ë¦„ ì‚¬ìš©

---

**ì°¸ê³  ë¬¸ì„œ**:
- **Ollama ê³µì‹ ë¬¸ì„œ**: https://docs.ollama.com/api/introduction
- `docs/PROJECT_API_SUMMARY.md` - API ëª…ì„¸ì„œ
- `docs/FINALFINAL.md` - í†µí•© ëª…ì„¸ì„œ
- `docs/Backend_í”„ë¡œì íŠ¸_í˜„í™©_ëª…ì„¸ì„œ.md` - Backend êµ¬í˜„ ëª…ì„¸ì„œ

---

## 7. ë³€ê²½ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ ë‚´ìš© |
|------|------|----------|
| 1.3 | 2026-01-26 | ë¡œê·¸ì¸/ë¡œê·¸ì•„ì›ƒ ì‹œìŠ¤í…œ í†µí•© ì™„ë£Œ ë°˜ì˜ - Backend JWT HttpOnly Cookie ì„¤ì •, Frontend NextAuth.js ì œê±° |
| 1.2 | 2026-01-26 | Ollama ê¸°ì¤€ìœ¼ë¡œ êµ¬í˜„ ë³€ê²½ - vLLM ì½”ë“œ ì£¼ì„ ì²˜ë¦¬, Ollamaë¥¼ ê¸°ë³¸ LLM ì„œë¹„ìŠ¤ë¡œ ì„¤ì • (LLM_SERVICE ê¸°ë³¸ê°’: ollama), GPU_SERVER_URL ì œê±°, chat.py Ollama ê¸°ì¤€ìœ¼ë¡œ ì¬êµ¬í˜„, ëª¨ë“  ë¬¸ì„œ ì—…ë°ì´íŠ¸ |
| 1.1 | 2026-01-26 | ë¬¸ì„œ ì •í•©ì„± ì‘ì—… - ì •ë³´ ì¼ê´€ì„± í™•ì¸ (ê¸°ì¤€ ë¬¸ì„œ) |
| 1.0 | 2026-01-26 | ì´ˆê¸° ë¬¸ì„œ ì‘ì„± |
